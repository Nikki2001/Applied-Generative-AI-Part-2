{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 9: Practical - Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We start with the same data preparation steps as in Module 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20761a90-0b44-468c-b4bd-2e5fb5c53411",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbdf654abb14a67bca4641f0161e311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikki\\sps_genai_v2\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:121: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nikki\\.cache\\huggingface\\hub\\datasets--rajpurkar--squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  # Likely running on Windows\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4fea644d6c4aaba5b2c5af411a7a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e25d3b17cf44beaa9113a1558621382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bce30c3032b40fea08f00d1710bc5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2115fbafce694e4dab1edc2daa0c24d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM  \n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").to(device)\n",
    "dataset = load_dataset(\"rajpurkar/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff27406-a8ca-40f3-a87d-0c6ec96a2589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "Xref",
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteEntryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-692f0b63-11a8729d2dd056c07f9f5d24;90c6866c-61b5-47d4-ac0c-0decc80630fe)\n\nEntry Not Found for url: https://huggingface.co/api/models/openai-community/gpt2/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sps_genai_v2\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:553\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_curlify\u001b[39m(request: requests.PreparedRequest) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert a `requests.PreparedRequest` into a curl command (str).\u001b[39;00m\n\u001b[32m    554\u001b[39m \n\u001b[32m    555\u001b[39m \u001b[33;03m    Used for debug purposes only.\u001b[39;00m\n\u001b[32m    556\u001b[39m \n\u001b[32m    557\u001b[39m \u001b[33;03m    Implementation vendored from https://github.com/ofw/curlify/blob/master/curlify.py.\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[33;03m    MIT License Copyright (c) 2016 Egor.\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    560\u001b[39m     parts: List[Tuple[Any, Any]] = [\n\u001b[32m    561\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mcurl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    562\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33m-X\u001b[39m\u001b[33m\"\u001b[39m, request.method),\n\u001b[32m    563\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sps_genai_v2\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '404 Not Found' for url 'https://huggingface.co/api/models/openai-community/gpt2/tree/main/additional_chat_templates?recursive=false&expand=false'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRemoteEntryNotFoundError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tokenizer = \u001b[43mGPT2Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopenai-community/gpt2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create sequences\u001b[39;00m\n\u001b[32m      5\u001b[39m SEQ_LEN = \u001b[32m30\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sps_genai_v2\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2039\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2035\u001b[39m             vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m   2036\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2037\u001b[39m             )\n\u001b[32m   2038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2039\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2045\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2046\u001b[39m         template = template.removesuffix(\u001b[33m\"\u001b[39m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2047\u001b[39m         vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sps_genai_v2\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:169\u001b[39m, in \u001b[36mlist_repo_templates\u001b[39m\u001b[34m(repo_id, local_files_only, revision, cache_dir, token)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveprefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sps_genai_v2\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3082\u001b[39m, in \u001b[36mlist_repo_tree\u001b[39m\u001b[34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[39m\n\u001b[32m   3062\u001b[39m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[32m   3063\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_repo_tree\u001b[39m(\n\u001b[32m   3064\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3072\u001b[39m     token: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3073\u001b[39m ) -> Iterable[Union[RepoFile, RepoFolder]]:\n\u001b[32m   3074\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3075\u001b[39m \u001b[33;03m    List a repo tree's files and folders and get information about them.\u001b[39;00m\n\u001b[32m   3076\u001b[39m \n\u001b[32m   3077\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   3078\u001b[39m \u001b[33;03m        repo_id (`str`):\u001b[39;00m\n\u001b[32m   3079\u001b[39m \u001b[33;03m            A namespace (user or an organization) and a repo name separated by a `/`.\u001b[39;00m\n\u001b[32m   3080\u001b[39m \u001b[33;03m        path_in_repo (`str`, *optional*):\u001b[39;00m\n\u001b[32m   3081\u001b[39m \u001b[33;03m            Relative path of the tree (folder) in the repo, for example:\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m \u001b[33;03m            `\"checkpoints/1fec34a/results\"`. Will default to the root tree (folder) of the repository.\u001b[39;00m\n\u001b[32m   3083\u001b[39m \u001b[33;03m        recursive (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[33;03m            Whether to list tree's files and folders recursively.\u001b[39;00m\n\u001b[32m   3085\u001b[39m \u001b[33;03m        expand (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[32m   3086\u001b[39m \u001b[33;03m            Whether to fetch more information about the tree's files and folders (e.g. last commit and files' security scan results). This\u001b[39;00m\n\u001b[32m   3087\u001b[39m \u001b[33;03m            operation is more expensive for the server so only 50 results are returned per page (instead of 1000).\u001b[39;00m\n\u001b[32m   3088\u001b[39m \u001b[33;03m            As pagination is implemented in `huggingface_hub`, this is transparent for you except for the time it\u001b[39;00m\n\u001b[32m   3089\u001b[39m \u001b[33;03m            takes to get the results.\u001b[39;00m\n\u001b[32m   3090\u001b[39m \u001b[33;03m        revision (`str`, *optional*):\u001b[39;00m\n\u001b[32m   3091\u001b[39m \u001b[33;03m            The revision of the repository from which to get the tree. Defaults to `\"main\"` branch.\u001b[39;00m\n\u001b[32m   3092\u001b[39m \u001b[33;03m        repo_type (`str`, *optional*):\u001b[39;00m\n\u001b[32m   3093\u001b[39m \u001b[33;03m            The type of the repository from which to get the tree (`\"model\"`, `\"dataset\"` or `\"space\"`.\u001b[39;00m\n\u001b[32m   3094\u001b[39m \u001b[33;03m            Defaults to `\"model\"`.\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[33;03m        token (`bool` or `str`, *optional*):\u001b[39;00m\n\u001b[32m   3096\u001b[39m \u001b[33;03m            A valid user access token (string). Defaults to the locally saved\u001b[39;00m\n\u001b[32m   3097\u001b[39m \u001b[33;03m            token, which is the recommended method for authentication (see\u001b[39;00m\n\u001b[32m   3098\u001b[39m \u001b[33;03m            https://huggingface.co/docs/huggingface_hub/quick-start#authentication).\u001b[39;00m\n\u001b[32m   3099\u001b[39m \u001b[33;03m            To disable authentication, pass `False`.\u001b[39;00m\n\u001b[32m   3100\u001b[39m \n\u001b[32m   3101\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   3102\u001b[39m \u001b[33;03m        `Iterable[Union[RepoFile, RepoFolder]]`:\u001b[39;00m\n\u001b[32m   3103\u001b[39m \u001b[33;03m            The information about the tree's files and folders, as an iterable of [`RepoFile`] and [`RepoFolder`] objects. The order of the files and folders is\u001b[39;00m\n\u001b[32m   3104\u001b[39m \u001b[33;03m            not guaranteed.\u001b[39;00m\n\u001b[32m   3105\u001b[39m \n\u001b[32m   3106\u001b[39m \u001b[33;03m    Raises:\u001b[39;00m\n\u001b[32m   3107\u001b[39m \u001b[33;03m        [`~utils.RepositoryNotFoundError`]:\u001b[39;00m\n\u001b[32m   3108\u001b[39m \u001b[33;03m            If repository is not found (error 404): wrong repo_id/repo_type, private but not authenticated or repo\u001b[39;00m\n\u001b[32m   3109\u001b[39m \u001b[33;03m            does not exist.\u001b[39;00m\n\u001b[32m   3110\u001b[39m \u001b[33;03m        [`~utils.RevisionNotFoundError`]:\u001b[39;00m\n\u001b[32m   3111\u001b[39m \u001b[33;03m            If revision is not found (error 404) on the repo.\u001b[39;00m\n\u001b[32m   3112\u001b[39m \u001b[33;03m        [`~utils.EntryNotFoundError`]:\u001b[39;00m\n\u001b[32m   3113\u001b[39m \u001b[33;03m            If the tree (folder) does not exist (error 404) on the repo.\u001b[39;00m\n\u001b[32m   3114\u001b[39m \n\u001b[32m   3115\u001b[39m \u001b[33;03m    Examples:\u001b[39;00m\n\u001b[32m   3116\u001b[39m \n\u001b[32m   3117\u001b[39m \u001b[33;03m        Get information about a repo's tree.\u001b[39;00m\n\u001b[32m   3118\u001b[39m \u001b[33;03m        ```py\u001b[39;00m\n\u001b[32m   3119\u001b[39m \u001b[33;03m        >>> from huggingface_hub import list_repo_tree\u001b[39;00m\n\u001b[32m   3120\u001b[39m \u001b[33;03m        >>> repo_tree = list_repo_tree(\"lysandre/arxiv-nlp\")\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m        >>> repo_tree\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[33;03m        <generator object HfApi.list_repo_tree at 0x7fa4088e1ac0>\u001b[39;00m\n\u001b[32m   3123\u001b[39m \u001b[33;03m        >>> list(repo_tree)\u001b[39;00m\n\u001b[32m   3124\u001b[39m \u001b[33;03m        [\u001b[39;00m\n\u001b[32m   3125\u001b[39m \u001b[33;03m            RepoFile(path='.gitattributes', size=391, blob_id='ae8c63daedbd4206d7d40126955d4e6ab1c80f8f', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[32m   3126\u001b[39m \u001b[33;03m            RepoFile(path='README.md', size=391, blob_id='43bd404b159de6fba7c2f4d3264347668d43af25', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[32m   3127\u001b[39m \u001b[33;03m            RepoFile(path='config.json', size=554, blob_id='2f9618c3a19b9a61add74f70bfb121335aeef666', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[32m   3128\u001b[39m \u001b[33;03m            RepoFile(\u001b[39;00m\n\u001b[32m   3129\u001b[39m \u001b[33;03m                path='flax_model.msgpack', size=497764107, blob_id='8095a62ccb4d806da7666fcda07467e2d150218e',\u001b[39;00m\n\u001b[32m   3130\u001b[39m \u001b[33;03m                lfs={'size': 497764107, 'sha256': 'd88b0d6a6ff9c3f8151f9d3228f57092aaea997f09af009eefd7373a77b5abb9', 'pointer_size': 134}, last_commit=None, security=None\u001b[39;00m\n\u001b[32m   3131\u001b[39m \u001b[33;03m            ),\u001b[39;00m\n\u001b[32m   3132\u001b[39m \u001b[33;03m            RepoFile(path='merges.txt', size=456318, blob_id='226b0752cac7789c48f0cb3ec53eda48b7be36cc', lfs=None, last_commit=None, security=None),\u001b[39;00m\n\u001b[32m   3133\u001b[39m \u001b[33;03m            RepoFile(\u001b[39;00m\n\u001b[32m   3134\u001b[39m \u001b[33;03m                path='pytorch_model.bin', size=548123560, blob_id='64eaa9c526867e404b68f2c5d66fd78e27026523',\u001b[39;00m\n\u001b[32m   3135\u001b[39m \u001b[33;03m                lfs={'size': 548123560, 'sha256': '9be78edb5b928eba33aa88f431551348f7466ba9f5ef3daf1d552398722a5436', 'pointer_size': 134}, last_commit=None, security=None\u001b[39;00m\n\u001b[32m   3136\u001b[39m \u001b[33;03m            ),\u001b[39;00m\n\u001b[32m   3137\u001b[39m \u001b[33;03m            RepoFile(path='vocab.json', size=898669, blob_id='b00361fece0387ca34b4b8b8539ed830d644dbeb', lfs=None, last_commit=None, security=None)]\u001b[39;00m\n\u001b[32m   3138\u001b[39m \u001b[33;03m        ]\u001b[39;00m\n\u001b[32m   3139\u001b[39m \u001b[33;03m        ```\u001b[39;00m\n\u001b[32m   3140\u001b[39m \n\u001b[32m   3141\u001b[39m \u001b[33;03m        Get even more information about a repo's tree (last commit and files' security scan results)\u001b[39;00m\n\u001b[32m   3142\u001b[39m \u001b[33;03m        ```py\u001b[39;00m\n\u001b[32m   3143\u001b[39m \u001b[33;03m        >>> from huggingface_hub import list_repo_tree\u001b[39;00m\n\u001b[32m   3144\u001b[39m \u001b[33;03m        >>> repo_tree = list_repo_tree(\"prompthero/openjourney-v4\", expand=True)\u001b[39;00m\n\u001b[32m   3145\u001b[39m \u001b[33;03m        >>> list(repo_tree)\u001b[39;00m\n\u001b[32m   3146\u001b[39m \u001b[33;03m        [\u001b[39;00m\n\u001b[32m   3147\u001b[39m \u001b[33;03m            RepoFolder(\u001b[39;00m\n\u001b[32m   3148\u001b[39m \u001b[33;03m                path='feature_extractor',\u001b[39;00m\n\u001b[32m   3149\u001b[39m \u001b[33;03m                tree_id='aa536c4ea18073388b5b0bc791057a7296a00398',\u001b[39;00m\n\u001b[32m   3150\u001b[39m \u001b[33;03m                last_commit={\u001b[39;00m\n\u001b[32m   3151\u001b[39m \u001b[33;03m                    'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\u001b[39;00m\n\u001b[32m   3152\u001b[39m \u001b[33;03m                    'title': 'Upload diffusers weights (#48)',\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[33;03m                    'date': datetime.datetime(2023, 3, 21, 9, 5, 27, tzinfo=datetime.timezone.utc)\u001b[39;00m\n\u001b[32m   3154\u001b[39m \u001b[33;03m                }\u001b[39;00m\n\u001b[32m   3155\u001b[39m \u001b[33;03m            ),\u001b[39;00m\n\u001b[32m   3156\u001b[39m \u001b[33;03m            RepoFolder(\u001b[39;00m\n\u001b[32m   3157\u001b[39m \u001b[33;03m                path='safety_checker',\u001b[39;00m\n\u001b[32m   3158\u001b[39m \u001b[33;03m                tree_id='65aef9d787e5557373fdf714d6c34d4fcdd70440',\u001b[39;00m\n\u001b[32m   3159\u001b[39m \u001b[33;03m                last_commit={\u001b[39;00m\n\u001b[32m   3160\u001b[39m \u001b[33;03m                    'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\u001b[39;00m\n\u001b[32m   3161\u001b[39m \u001b[33;03m                    'title': 'Upload diffusers weights (#48)',\u001b[39;00m\n\u001b[32m   3162\u001b[39m \u001b[33;03m                    'date': datetime.datetime(2023, 3, 21, 9, 5, 27, tzinfo=datetime.timezone.utc)\u001b[39;00m\n\u001b[32m   3163\u001b[39m \u001b[33;03m                }\u001b[39;00m\n\u001b[32m   3164\u001b[39m \u001b[33;03m            ),\u001b[39;00m\n\u001b[32m   3165\u001b[39m \u001b[33;03m            RepoFile(\u001b[39;00m\n\u001b[32m   3166\u001b[39m \u001b[33;03m                path='model_index.json',\u001b[39;00m\n\u001b[32m   3167\u001b[39m \u001b[33;03m                size=582,\u001b[39;00m\n\u001b[32m   3168\u001b[39m \u001b[33;03m                blob_id='d3d7c1e8c3e78eeb1640b8e2041ee256e24c9ee1',\u001b[39;00m\n\u001b[32m   3169\u001b[39m \u001b[33;03m                lfs=None,\u001b[39;00m\n\u001b[32m   3170\u001b[39m \u001b[33;03m                last_commit={\u001b[39;00m\n\u001b[32m   3171\u001b[39m \u001b[33;03m                    'oid': 'b195ed2d503f3eb29637050a886d77bd81d35f0e',\u001b[39;00m\n\u001b[32m   3172\u001b[39m \u001b[33;03m                    'title': 'Fix deprecation warning by changing `CLIPFeatureExtractor` to `CLIPImageProcessor`. (#54)',\u001b[39;00m\n\u001b[32m   3173\u001b[39m \u001b[33;03m                    'date': datetime.datetime(2023, 5, 15, 21, 41, 59, tzinfo=datetime.timezone.utc)\u001b[39;00m\n\u001b[32m   3174\u001b[39m \u001b[33;03m                },\u001b[39;00m\n\u001b[32m   3175\u001b[39m \u001b[33;03m                security={\u001b[39;00m\n\u001b[32m   3176\u001b[39m \u001b[33;03m                    'safe': True,\u001b[39;00m\n\u001b[32m   3177\u001b[39m \u001b[33;03m                    'av_scan': {'virusFound': False, 'virusNames': None},\u001b[39;00m\n\u001b[32m   3178\u001b[39m \u001b[33;03m                    'pickle_import_scan': None\u001b[39;00m\n\u001b[32m   3179\u001b[39m \u001b[33;03m                }\u001b[39;00m\n\u001b[32m   3180\u001b[39m \u001b[33;03m            )\u001b[39;00m\n\u001b[32m   3181\u001b[39m \u001b[33;03m            ...\u001b[39;00m\n\u001b[32m   3182\u001b[39m \u001b[33;03m        ]\u001b[39;00m\n\u001b[32m   3183\u001b[39m \u001b[33;03m        ```\u001b[39;00m\n\u001b[32m   3184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3185\u001b[39m     repo_type = repo_type \u001b[38;5;129;01mor\u001b[39;00m constants.REPO_TYPE_MODEL\n\u001b[32m   3186\u001b[39m     revision = quote(revision, safe=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m constants.DEFAULT_REVISION\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sps_genai_v2\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_pagination.py:37\u001b[39m, in \u001b[36mpaginate\u001b[39m\u001b[34m(path, params, headers)\u001b[39m\n\u001b[32m     35\u001b[39m session = get_session()\n\u001b[32m     36\u001b[39m r = session.get(path, params=params, headers=headers)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m r.json()\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Follow pages\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Next link already contains query params\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sps_genai_v2\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:567\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(request.headers.items()):\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m k.lower() == \u001b[33m\"\u001b[39m\u001b[33mauthorization\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m         v = \u001b[33m\"\u001b[39m\u001b[33m<TOKEN>\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Hide authorization header, no matter its value (can be Bearer, Key, etc.)\u001b[39;00m\n\u001b[32m    568\u001b[39m     parts += [(\u001b[33m\"\u001b[39m\u001b[33m-H\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m.format(k, v))]\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.body:\n",
      "\u001b[31mRemoteEntryNotFoundError\u001b[39m: 404 Client Error. (Request ID: Root=1-692f0b63-11a8729d2dd056c07f9f5d24;90c6866c-61b5-47d4-ac0c-0decc80630fe)\n\nEntry Not Found for url: https://huggingface.co/api/models/openai-community/gpt2/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\""
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('openai-community/gpt2',use_fast=True,local_files_only=False)\n",
    "\n",
    "# Create sequences\n",
    "SEQ_LEN = 30\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset, split='train'):\n",
    "        self.data = dataset[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - SEQ_LEN\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        prompt = f\"Question: {example['question']}\\nContext: {example['context']}\\nAnswer:\"\n",
    "        answer = f\"{prompt} {example['answers']['text'][0]}\"\n",
    "        return prompt, answer\n",
    "        #return self.data[idx]\n",
    "            #torch.tensor(self.data[idx+1:idx+SEQ_LEN+1]))\n",
    "\n",
    "train_datasets = TextDataset(dataset)\n",
    "print(train_datasets[0])\n",
    "train_loader = DataLoader(train_datasets, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's see what the first pair of input/output sequences look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We now define the causal attention mask.  Recall that this mask simply zeroes out the attention weights for future tokens in the sequence. This is done to ensure that the model does not have access to future tokens when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(n_dest, n_src, device):\n",
    "    i = torch.arange(n_dest, device=device).unsqueeze(1)\n",
    "    j = torch.arange(n_src, device=device).unsqueeze(0)\n",
    "    return i >= j\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "mask = causal_attention_mask(10, 10, device)\n",
    "print(mask[0].T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Recall that we also need to define a position embedding.  Here we will use a simple positional encoding corresponding to the embedding of the index of the token in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Next we define the Transformer block, consisting of, in addition to the usual fully connected layers, also multi-head attention and layer normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_gpt(model, dataloader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        data_loader_with_progress = tqdm(\n",
    "            iterable=dataloader, ncols=120, desc=f\"Epoch {epoch+1}/{epochs}\"\n",
    "        )\n",
    "        for batch_number, (inputs, targets) in enumerate(data_loader_with_progress):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(inputs)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if (batch_number % 100 == 0) or (batch_number == len(dataloader) - 1):\n",
    "                data_loader_with_progress.set_postfix(\n",
    "                    {\n",
    "                        \"avg loss\": f\"{total_loss/(batch_number+1):.4f}\",\n",
    "                    }\n",
    "                )            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We can now use the trained GPT to generate text.  The model will generate a sequence of tokens based on the input prompt. We can use the inverse mapping from our vocabulary to \"translate\" the tokens to natural text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, model, index_to_word, top_k=10):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(index_to_word)}\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs[1] = 0  # Mask out UNK token (index 1) to prevent generating <UNK>\n",
    "        probs = torch.nn.functional.softmax(probs/temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        return next_id, probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temperature):\n",
    "        self.model.eval()\n",
    "        start_tokens = [self.word_to_index.get(w, 1) for w in start_prompt.split()]\n",
    "        generated_tokens = start_tokens[:]\n",
    "        info = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while len(generated_tokens) < max_tokens:\n",
    "                x = torch.tensor([generated_tokens], dtype=torch.long)\n",
    "                x = x.to(device)\n",
    "                logits, attn_weights = self.model(x)\n",
    "                last_logits = logits[0, -1] # .cpu().numpy()\n",
    "                sample_token, probs = self.sample_from(last_logits, temperature)\n",
    "                generated_tokens.append(sample_token)\n",
    "                info.append({\n",
    "                    \"prompt\": start_prompt,\n",
    "                    \"word_probs\": probs,\n",
    "                    \"atts\": attn_weights[0].cpu().numpy()\n",
    "                })\n",
    "                if sample_token == 0:\n",
    "                    break\n",
    "        print(\"GEN\", generated_tokens)\n",
    "        generated_words = [self.index_to_word.get(idx, \"<UNK>\") for idx in generated_tokens]\n",
    "        print(\"generated text:\" + \" \".join(generated_words))\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = TextGenerator(model, inv_vocab)\n",
    "info = text_generator.generate(\"captain \", max_tokens=180, temperature=3.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
