{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88ca633b-5014-4520-aa62-4262769dcd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM  \n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").to(device)\n",
    "dataset = load_dataset(\"rajpurkar/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cff27406-a8ca-40f3-a87d-0c6ec96a2589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43bafaa3-7057-4621-a008-64e3726faa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([24361,    25,   554,   644,   614,   750,   262,  1074,  1085,   416,\n",
      "         6102,  1133,  4631,   710,  1592,   262,  8049,  8693,    30,   198,\n",
      "        21947,    25,  1881,   286,   262,  1388,  5059,  3386,   287,   262,\n",
      "         3349,   286,   262,  2059,   373,   663,  4346,  1074,    11,   262,\n",
      "        23382, 20377, 19098,  8685,    13,  6102,  1133,  4631,   710,  2627,\n",
      "         1182,  3985,   287, 25859,    13,  4698,  4631,   710,    11,   262,\n",
      "         8685,   561,  1281,   257,  1700,   286, 13343,  7864,    11,  1105,\n",
      "         9089,    11,   290,  1936,  8470,    13,  5856,   465,  1511,   812,\n",
      "          262,  8685,  1839,  1115,  2260, 27459,    11,   550,  1936, 41445,\n",
      "         7028,    11,  1839,   262,  8049,  8693,   287, 36864,    11,   290,\n",
      "         4635,  1938,   884,   355,  4502,   402,  3974,   290,   262,   366,\n",
      "        15137, 18455,  3653,  1911,  6102,  1133,  4631,   710,   468,   262,\n",
      "         4511,  5442,  5873, 20262,  3459,    16,     8,   287, 15244,  7458,\n",
      "          314,    14,    37,  4462,  4346,  2106,    13,  4631,   710,   338,\n",
      "        18514,  9322,   262, 23382, 20377,  8315,   290,   465, 18370,  4966]), tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikki\\sps_genai_v2\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2data')\n",
    "\n",
    "# Create sequences\n",
    "SEQ_LEN = 150\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset, split='train'):\n",
    "        self.data = dataset[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        prompt = f\"Question: {example['question']}\\nContext: {example['context']}\\nAnswer:\"\n",
    "        answer = f\"{prompt} {example['answers']['text'][0]}\"\n",
    "        \n",
    "        prompt_encodings = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=SEQ_LEN,\n",
    "            padding = False,\n",
    "            return_tensors=None\n",
    "        ).input_ids\n",
    "\n",
    "        answer_encodings = tokenizer(\n",
    "            answer,\n",
    "            truncation=True,\n",
    "            max_length=SEQ_LEN,\n",
    "            padding = False,\n",
    "            return_tensors=None\n",
    "        ).input_ids\n",
    "\n",
    "        inputs = prompt_encodings + [tokenizer.eos_token_id] * (SEQ_LEN - len(prompt_encodings))\n",
    "        # Later set -100 token to be ignored when calculating the loss\n",
    "        labels = [-100] * len(prompt_encodings) + answer_encodings[len(prompt_encodings):] + [-100] * (SEQ_LEN - len(answer_encodings))\n",
    "       \n",
    "        mask = torch.cat([\n",
    "            torch.ones(len(prompt_encodings), dtype=torch.long),\n",
    "            torch.zeros(SEQ_LEN - len(prompt_encodings), dtype=torch.long)\n",
    "        ])\n",
    "       \n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(labels, dtype=torch.long), mask\n",
    "\n",
    "train_datasets = TextDataset(dataset)\n",
    "print(train_datasets[100])\n",
    "train_loader = DataLoader(train_datasets, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's see what the first pair of input/output sequences look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "BYtC",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[24361,    25,  6350,  ..., 50256, 50256, 50256],\n",
       "         [24361,    25,  1867,  ...,  1678,   357,    67],\n",
       "         [24361,    25, 28470,  ..., 50256, 50256, 50256],\n",
       "         ...,\n",
       "         [24361,    25,  1867,  ..., 50256, 50256, 50256],\n",
       "         [24361,    25,   554,  ..., 50256, 50256, 50256],\n",
       "         [24361,    25,  1867,  ...,    72,    12,    33]]),\n",
       " tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_gpt(model, dataloader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        data_loader_with_progress = tqdm(\n",
    "            iterable=dataloader, ncols=120, desc=f\"Epoch {epoch+1}/{epochs}\"\n",
    "        )\n",
    "        for batch_number, (inputs, targets, mask) in enumerate(data_loader_with_progress):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits= model(inputs, attention_mask=mask).logits\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if (batch_number % 100 == 0) or (batch_number == len(dataloader) - 1):\n",
    "                data_loader_with_progress.set_postfix(\n",
    "                    {\n",
    "                        \"avg loss\": f\"{total_loss/(batch_number+1):.4f}\",\n",
    "                    }\n",
    "                )            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739a10d-ebf9-4307-b741-14f78712ef3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|                                                                               | 0/1369 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-100) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_gpt(model, train_loader, optimizer, criterion, epochs=2, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We can now use the trained GPT to generate text.  The model will generate a sequence of tokens based on the input prompt. We can use the inverse mapping from our vocabulary to \"translate\" the tokens to natural text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, model, top_k=10):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs[1] = 0  # Mask out UNK token (index 1) to prevent generating <UNK>\n",
    "        probs = torch.nn.functional.softmax(probs/temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        return next_id, probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temperature):\n",
    "        self.model.eval()\n",
    "        generated_tokens = tokenizer(\n",
    "            start_prompt,\n",
    "            truncation=True,\n",
    "            max_length=SEQ_LEN,\n",
    "            padding = False,\n",
    "            return_tensors=None\n",
    "        ).input_ids\n",
    "\n",
    "        info = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while len(generated_tokens) < max_tokens:\n",
    "                x = torch.tensor([generated_tokens], dtype=torch.long)\n",
    "                x = x.to(device)\n",
    "                logits = self.model(x).logits\n",
    "                last_logits = logits[0, -1] # .cpu().numpy()\n",
    "                sample_token, probs = self.sample_from(last_logits, temperature)\n",
    "                generated_tokens.append(sample_token)\n",
    "                info.append({\n",
    "                    \"prompt\": start_prompt,\n",
    "                    \"word_probs\": probs,\n",
    "                })\n",
    "                if sample_token == 0:\n",
    "                    break\n",
    "        print(\"GEN\", generated_tokens)\n",
    "        generated_words = tokenizer.decode(generated_tokens)\n",
    "        print(\"generated text:\" + \" \".join(generated_words))\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = TextGenerator(model)\n",
    "info = text_generator.generate(\"captain \", max_tokens=150, temperature=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb94642-deac-4d78-a87f-51077458a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint every epoch\n",
    "import os\n",
    "\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}\n",
    "\n",
    "# Save latest checkpoint\n",
    "checkpoint_path = os.path.join(\"gpt2_checkpoints\", f'gpt2.pth')\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Checkpoint saved: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe80f6-c539-41a3-96d6-87fcaacb01a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
